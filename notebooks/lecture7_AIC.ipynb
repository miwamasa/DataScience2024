{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pubs.acs.org/doi/suppl/10.1021/ci034243x/suppl_file/ci034243xsi20040112_053635.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture7 AIC、多重共線性、AICをつかった変数選択の例\n",
    "\n",
    "参考：[pythonのstatsmodelsを使った重回帰分析で溶解度予測：AICによるモデル選択](https://future-chem.com/esol-reg-aic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの準備\n",
    "\n",
    "データは、化学論文の化合物の溶解度予測で餅られているデータを使って、AIC、多重共線性、AICを使った変数選択（モデル選択）の例を示します。\n",
    "\n",
    "データはSMILESと呼ばれるテキスト形式で示された分子構造（図）から、あらかじめ特量を計算したものを準備しています。\n",
    "\n",
    "データセットの引用元であるEstimated SOLubility（ESOL）の論文では，\n",
    "\n",
    "- clogP\n",
    "- 分子量：MW\n",
    "- 回転可能結合数：RB\n",
    "- 芳香族指数：AP\n",
    "- 非炭素原子数の比：NC\n",
    "- 水素結合ドナー数：HD\n",
    "- 水素結合アクセプター数：HA\n",
    "- 極性表面積：PSA\n",
    "  \n",
    "の8つの記述子を用いて重回帰分析を行い，「各パラメータの重要さをt値の絶対値によって評価」することで最初の4つの記述子に絞り込み，最終的に以下の回帰式を得ています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ライブラリのインポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/miwamasa/DataScience2023/main/notebooks/data/ESOL_paper.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['logP', 'MW', 'RB', 'AP', 'NC', 'HA', 'HD', 'TPSA']],\n",
    "                                                   df.m_sol, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# ((858, 8), (286, 8), (858,), (286,))\n",
    "fig = plt.figure()\n",
    "sns.heatmap(data=X_train.corr().round(2), annot=True, vmax=1, vmin=-1, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodelsを用いたモデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULLモデルの作成\n",
    "\n",
    "8個全ての記述子をモデル作成に使用してみましょう．statsmodelsではデータが全て同一のpandasデータフレーム中にあることが前提となっていますので，まずX_trainとy_trainをまとめて1つのデータフレームにしています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_train = X_train.join(y_train)\n",
    "full_model = smf.ols('m_sol ~ logP + MW + RB + AP + NC + HA + HD + TPSA', data=X_y_train).fit()\n",
    "full_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表中のt値やp値の意味は，「記述子の係数がゼロであるモデル（帰無仮説）」に対して，係数がゼロではない確率をt検定した値になります．つまりp値が低ければモデルに意味のある情報を加えている記述子と理解してよいです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOLモデルの作成\n",
    "「logP」「MW」「RB」「AP」の４つの記述子を用いたモデルの作成もしてみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esol_model = smf.ols('m_sol ~ logP + MW + RB + AP', data=X_y_train).fit()\n",
    "print(esol_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も乱数の種を同じに設定したため，全く同じ式が得られました．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多重共線性（multicollinearity）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多重共線性とは，説明変数間で相関係数が高い場合に生じる現象です．相関の高い複数の変数は同じような情報を持っているため，作成したモデルにおける係数の大小や符号などの解釈が難しくなってしまいます．一般的には相関係数が0.8を超える場合には注意が必要と言われています．先ほどの調べたようにPTSAとHAの相関係数が0.9でしたので，注意が必要そうです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分散拡大要因（VIF：Variance Inflation Factor）\n",
    "\n",
    "独立変数間の相関係数行列の逆行列の対角要素を分散拡大要因（VIF）と呼び，多重共線性の存在をチェックするために使われます．VIFが10を超える独立変数は，多重共線性の原因となる可能性が高く，モデル構築の際には除去する方が好ましいと言われます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = np.array(X_train.corr())\n",
    "inv_corr_mat = np.linalg.inv(corr_mat)\n",
    "pd.Series(np.diag(inv_corr_mat), index=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPSAのVIFが10を超えているため，モデル構築の際には取り除くほうがよさそうです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 赤池情報量規準：AIC\n",
    "\n",
    "は「赤池情報量規準（AIC：Akaike’s Information Criterion）」と呼ばれるものを用いてモデル選択を行っていこうと思います．類似の多数ありますが，AICが最もよく使われます．\n",
    "\n",
    "AICは\n",
    "\n",
    "AIC=−2×(最大対数尤度–推定されたパラメータ数）\n",
    "で表される指標で，AICが小さいほど「よい」モデルとみなされます．尤度はパラメータ数を増やすと大きくなりやすいため，用いるパラメータ数に罰則をつけています．そのため不必要なパラメータが含まれていないモデルが選ばれやすいと言えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esol_model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodelsを用いた重回帰分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全探索\n",
    "\n",
    "今回の場合はTPSAを除くと記述子の数が7個ですので，全ての組み合わせを考えて最も小さいAICを与えるモデルを選択することが現実的です．pythonで組み合わせを考える場合にはitertools.combinationsを用いるのが便利です．下記のコードでは\n",
    "\n",
    "- generate_formulas関数で記述子の組合せから作成するモデルのformulaを生成\n",
    "- modeling_linear関数でformulaに従ってモデルを作成\n",
    "- 記述子ゼロのNullモデルを別個作成，評価\n",
    "- for文中で利用する記述しを変化させながらモデルを作成，評価\n",
    "- AICの値でソート\n",
    "\n",
    "という順番で処理をしています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list = ['logP', 'MW', 'RB', 'AP', 'NC', 'HA', 'HD']\n",
    "\n",
    "def generate_formulas(descriptors, num):\n",
    "    import itertools\n",
    "    if num > len(descriptors):\n",
    "        return None\n",
    "    else:\n",
    "        for f in itertools.combinations(descriptors, num):\n",
    "            formula = 'm_sol ~ ' + ' + '.join(f)\n",
    "            yield formula\n",
    "\n",
    "def modeling_linear(formula, data):\n",
    "    model = smf.ols(formula=formula, data=data).fit()\n",
    "    return formula, model\n",
    "\n",
    "n_var = []\n",
    "formula = []\n",
    "model = []\n",
    "AIC = []\n",
    "adj_r2 = []\n",
    "## Nullモデル作成\n",
    "null_model = smf.ols(formula='m_sol ~ 1', data=X_y_train).fit()\n",
    "n_var.append(0)\n",
    "formula.append('m_sol ~ 1')\n",
    "model.append(null_model)\n",
    "AIC.append(null_model.aic)\n",
    "adj_r2.append(null_model.rsquared_adj)\n",
    "## 利用する記述子の数を変えながらモデル作成\n",
    "for i in range(1,9):\n",
    "    for f in generate_formulas(desc_list, i):\n",
    "        eq, m = modeling_linear(formula=f, data=X_y_train)\n",
    "        n_var.append(i)\n",
    "        formula.append(f)\n",
    "        model.append(m)\n",
    "        AIC.append(m.aic)\n",
    "        adj_r2.append(m.rsquared_adj)\n",
    "\n",
    "df_model = pd.DataFrame({'n_var': n_var,\n",
    "                        'formula': formula,\n",
    "                        'model': model,\n",
    "                        'aic': AIC,\n",
    "                        'adj_r2': adj_r2})\n",
    "## AIC評価で上位5つ\n",
    "df_model.aic.sort_values()[:5]\n",
    "df_model.formula[df_model.aic.sort_values()[:5].index]\n",
    "print(df_model.model[df_model.aic.idxmin()].summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.aic.sort_values()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.formula[df_model.aic.sort_values()[:5].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これより，「logP」「MW」「AP」「NC」「HA」「HD」の6つを用いたモデルが最もAICが低かったようです．P値の値も大丈夫そうです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップワイズ法（stepwise regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステップワイズ法では全ての変数を組み入れたFullモデルを作成し，そこから1つの変数を取り除いたモデルを作成します（今回は7種類）．もっともAICの低いモデルを採用し，変数6個のモデルとします．そこからまた1つの変数を取り除いたモデルを作成し，，，と繰り返し改善が見られなくなった点で探索を終了します．逆に変数０のNullモデルから出発し，１つずつ変数を足していくステップワイズ法も考えられます．\n",
    "\n",
    "今回はFullモデルから始めるステップワイズ法を，R言語のstepAIC関数に類似したpython関数として実装します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepAIC(descs_l):\n",
    "    import copy\n",
    "    descriptors = descs_l\n",
    "    f_model = smf.ols(formula='m_sol ~ ' + ' + '.join(descriptors), data=X_y_train).fit()\n",
    "    best_aic = f_model.aic\n",
    "    best_model = f_model\n",
    "    while descriptors:\n",
    "        desc_selected = ''\n",
    "        flag = 0\n",
    "        for desk in descriptors:\n",
    "            used_desks = copy.deepcopy(descriptors)\n",
    "            used_desks.remove(desk)\n",
    "            formula = 'm_sol ~ ' + ' + '.join(used_desks)\n",
    "            model = smf.ols(formula=formula, data=X_y_train).fit()\n",
    "            if model.aic < best_aic:\n",
    "                best_aic = model.aic\n",
    "                best_model = model\n",
    "                desc_selected = desk\n",
    "                flag = 1\n",
    "        if flag:\n",
    "            descriptors.remove(desc_selected)\n",
    "        else:\n",
    "            break\n",
    "    return best_model\n",
    "\n",
    "model_a = stepAIC(desc_list)\n",
    "print(model_a.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作成したモデルの評価\n",
    "\n",
    "以下のコードではステップワイズ法で得たモデルの残差を可視化し，正規分布の確率密度関数と重ね合わせています．またscipy.statsに実装されているShapiro-Wilkの正規性検定による確認も行っています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "predict = model_a.predict()\n",
    "resid = model_a.resid\n",
    "rvs = stats.norm.pdf\n",
    "x = np.linspace(-5,5,100)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "sns.distplot(resid)\n",
    "ax1.plot(x, rvs(x), 'k--', lw=1)\n",
    "## Shapiro–Wilk test\n",
    "stats.shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またQ–Qプロットと呼ばれる，理論上の正規分布における分位点と実際のデータの分位点を散布図としてプロットしたグラフで確認することもあります．Qは「Quantile」の略になります．statsmodels.apiにはqqplotというそのものズバリな関数が実装されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.qqplot(resid, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残差は正規分布に従うと結論してよさそうです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
