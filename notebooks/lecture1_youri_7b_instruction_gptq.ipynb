{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture1:Colab（無料枠）で試せる、日本語LLM(youri7b,calm2-7b-chat)\n",
        "\n",
        "Pythonを使えば、ChatGPTに代表されるLLM(大規模言語モデル)を動かして動作を確認することもできます（計算機の性能により動作しい場合もある）。\n",
        "\n",
        "\n",
        "> 注）２つのモデルを切り替える際は「ランタイムを接続解除して削除」を行う必要がある場合があります。\n",
        "\n",
        "\n",
        "\n",
        "### rinna/youri-7b\n",
        "\n",
        "rinnaが開発した、meta社のLlama2_7Bをベースに日本語の学習データを用いて継続事前学習を行ったのが、汎用言語モデル「Youri 7B」です。\n",
        "\n",
        "さらに、汎用言語モデルであるYouri 7Bに、対話形式でユーザーの指示を遂行するような追加学習をしたモデルが「Youri 7B Instruction」です。日本語の一問一答に応える能力が高くベンチマークにおいて高いスコアを達成します。\n",
        "\n",
        "rinna/youri-7b-instruction-gptq　は、Youri 7B instructionを量子化してコンパクトにしたもので、これならば、日本語の性能がそそこでかつColab無料枠でも動作します。\n",
        "\n",
        "### cyberagent/calm2-7b-chat\n",
        "calm2-7b-chatは、CyberAgentが公開した日本語特化のLLM(大規模言語モデル)のopen-calmの対話モデルです。Colab無料枠で動かすために、量子化したmmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1kで動作を確認しています。\n",
        "\n"
      ],
      "metadata": {
        "id": "KiHGBj_un9pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/rinna/youri-7b-instruction-gptq\n"
      ],
      "metadata": {
        "id": "yWG0v-Y7v5Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install auto-gptq"
      ],
      "metadata": {
        "id": "HM8jHwGIvuLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VQPWb3mvmYn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "model_name = \"rinna/youri-7b-instruction-gptq\"\n",
        "model_name = \"mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name, use_safetensors=True)\n",
        "\n",
        "instruction = \"次の日本語を英語に翻訳してください。\"\n",
        "input = \"大規模言語モデル（だいきぼげんごモデル、英: large language model、LLM）は、多数のパラメータ（数千万から数十億）を持つ人工ニューラルネットワークで構成されるコンピュータ言語モデルで、膨大なラベルなしテキストを使用して自己教師あり学習または半教師あり学習によって訓練が行われる。\"\n",
        "prompt = f\"\"\"\n",
        "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
        "\n",
        "### 指示:\n",
        "{instruction}\n",
        "\n",
        "### 入力:\n",
        "{input}\n",
        "\n",
        "### 応答:\n",
        "\"\"\"\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids=token_ids.to(model.device),\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_responce(instruction, input):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
        "\n",
        "  ### 指示:\n",
        "  {instruction}\n",
        "\n",
        "  ### 入力:\n",
        "  {input}\n",
        "\n",
        "  ### 応答:\n",
        "  \"\"\"\n",
        "  token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output_ids = model.generate(\n",
        "          input_ids=token_ids.to(model.device),\n",
        "          max_new_tokens=200,\n",
        "          do_sample=True,\n",
        "          temperature=0.5,\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          bos_token_id=tokenizer.bos_token_id,\n",
        "          eos_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "\n",
        "  output = tokenizer.decode(output_ids.tolist()[0])\n",
        "  return output"
      ],
      "metadata": {
        "id": "-OFiI-7exUXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"あなたは優秀なAIです。的確に答えてください。\"\n",
        "input = \"500円のケーキを３人で食べました、それぞれいくら払うべき？\"\n",
        "\n",
        "res = get_responce(instruction, input)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "rpg0RW02x1IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"あなたは優秀なAIです。的確に答えてください。\"\n",
        "input = \"世界で一番高い山はなんですか？それは東京ドームより高いですか？\"\n",
        "\n",
        "res = get_responce(instruction, input)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "SIwPYr7AykkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"あなたは優秀なAIです。的確に答えてください。\"\n",
        "input = \"清少納言というのは正式な名前か？\"\n",
        "res = get_responce(instruction, input)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "RgU8Ar1WzNh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld-Opu4zxV4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}