{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# データサイエンス特論2024: Lecture14 LLM時代のデータサイエンスとは(RAG)\n",
        "## RAG(Retrieval Augmented Generation)の例\n",
        "\n",
        "\n",
        "(C)2024 岩政 幹人\n",
        "> 注：本試行を勝手に公開することを禁ず\n",
        "\n",
        "\n",
        "ChatGPTに代表される、大規模言語モデル(LLM)を活用した様々なタスクの実現が可能になってきました。\n",
        "\n",
        "ここでは、代表的なLLMの活用例として、特定の文章を対象にしたQ&Aを行うアプリケーションを想定します。\n",
        "\n",
        "RAG:Retrieval Augmented Generationと呼ばれる、LLMの利用パターンになります。\n",
        "\n",
        "以下ChatGPTの説明より、\n",
        "> 情報検索を行うリトリーバー（Retriever）と、検索された情報を元に文章を生成するジェネレーター（Generator）が組み合わされます。\n",
        "> 具体的には、リトリーバーは与えられた質問やテキストに関連する情報をデータベースや文書コレクションから検索し、その情報を抽出します。そして、ジェネレーターはリトリーバーが検索した情報を元に、より自然な形で回答や文章を生成します。\n",
        "\n",
        "元となるテキストの取得については以下を参考にしました。\n",
        "\n",
        "[LangChainを使ったRAGをElyza 7bを用いて試してみた](https://note.com/alexweberk/n/n3cffc010e9e9#98fafb11-8969-49b6-8990-fad204db936d)\n",
        "\n",
        "また、OpenAIを有料利用していない方でも試せるように、オープンソースのLLMを使います。\n",
        "\n",
        "\n",
        ">【注意】この環境は2024.5月に動作確認したものであり、利用するソフトのバージョンアップで使えなくなる可能性が高いことに注意。\n",
        "\n",
        "\n",
        "オープンソースLLMを利用する方法として、ここでは、いったんローカル環境(といってもcolaboratory計算機ですが）にLLMをダウンロードして利用するローカルLLMを利用します。情報が外部に漏れることを気にする方はローカルLLMは便利ですね。ここではOllamaというソフトウエアを用いて、オープンソースのLLMをローカル環境にダウンロードして活用します。\n",
        "\n"
      ],
      "metadata": {
        "id": "ZF5kguv6EQRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  利用環境\n",
        "- [Ollama](https://ollama.com/)\n",
        "- [llama_index](https://docs.llamaindex.ai/en/stable/)\n",
        "- [langchain](https://python.langchain.com/docs/get_started/introduction) 、0.2.1\n",
        "\n",
        "以下gemini様の説明より\n",
        "> LlamaIndexとLangChainは、どちらも大規模言語モデル（LLM）を活用するための強力なライブラリですが、<略>\n",
        "> LlamaIndexはQAシステムやデータインデックス作成に特化し、初心者でも扱いやすい簡潔さが魅力。一方、LangChainは汎用性の高いLLMフレームワークで、複雑なタスクや高度なカスタマイズにも対応。それぞれの強みを活かし、目的に合ったツールを選ぶことで、LLM活用の可能性が大きく広がります。\n",
        "\n",
        "以下copilot様より、、\n",
        "> Ollamaは、ユーザーがローカル環境で大規模な言語モデルを簡単に動作させることができるツールです。LLama2やCode Llamaなどのモデルを実行することができ、カスタマイズや独自のモデルの作成も可能です"
      ],
      "metadata": {
        "id": "OxFY4QbeO0IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ollamaをインストールします。\n"
      ],
      "metadata": {
        "id": "mbYOAxVclp6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!sudo apt install -y neofetch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:00:59.287969Z",
          "iopub.execute_input": "2024-03-06T16:00:59.288578Z",
          "iopub.status.idle": "2024-03-06T16:01:12.100284Z",
          "shell.execute_reply.started": "2024-03-06T16:00:59.288542Z",
          "shell.execute_reply": "2024-03-06T16:01:12.099035Z"
        },
        "trusted": true,
        "id": "OjdgkJhdEQRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "colabマシンのスペックを見ます。"
      ],
      "metadata": {
        "id": "LUR5JlOVhB52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!neofetch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:01:12.102689Z",
          "iopub.execute_input": "2024-03-06T16:01:12.103496Z",
          "iopub.status.idle": "2024-03-06T16:01:13.366218Z",
          "shell.execute_reply.started": "2024-03-06T16:01:12.103454Z",
          "shell.execute_reply": "2024-03-06T16:01:13.364847Z"
        },
        "trusted": true,
        "id": "qLHuhU1vEQRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ollama serverを起動\n",
        "\n",
        "colab環境にてollamaを使うためのサーバーを起動します。このサーバーは、外部から(pythonなどから）ollamaが管理するllmにアクセスるためのサーバーでもあります。\n",
        "\n",
        "ローカルLLMを使っていて、時々以下のようなエラーが出る場合は、ollama severが停止している可能性があるので、↓のセルをもう一度起動しましょう。\n",
        "\n",
        "> ConnectError: [Errno 99] Cannot assign requested address"
      ],
      "metadata": {
        "id": "oEHLd7GvEQRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama as a backrgound process\n",
        "command = \"nohup ollama serve&\"\n",
        "\n",
        "# Use subprocess.Popen to start the process in the background\n",
        "process = subprocess.Popen(command,\n",
        "                            shell=True,\n",
        "                           stdout=subprocess.PIPE,\n",
        "                           stderr=subprocess.PIPE)\n",
        "print(\"Process ID:\", process.pid+1)\n",
        "time.sleep(5)  # Makes Python wait for 5 seconds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:01:35.318770Z",
          "iopub.execute_input": "2024-03-06T16:01:35.319180Z",
          "iopub.status.idle": "2024-03-06T16:01:40.332324Z",
          "shell.execute_reply.started": "2024-03-06T16:01:35.319140Z",
          "shell.execute_reply": "2024-03-06T16:01:40.331386Z"
        },
        "trusted": true,
        "id": "unUG2gdKEQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ollama serveの稼働を確かめます\n",
        "\n",
        "- エラーがでるならば↑を再起動\n",
        "- 帰ってこないならば、\n",
        "\n",
        "\n",
        "```\n",
        "!kill  <↑のProcess ID>\n",
        "```\n",
        "\n",
        "の後で、ollama serveを起動します。\n",
        "\n"
      ],
      "metadata": {
        "id": "J_uHcwo3FCHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!kill 2137"
      ],
      "metadata": {
        "id": "-YQaM9zdK4qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "I0BlaWJLFFev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ここでollamaでモデルをダウンロードしてrunします。\n",
        "\n",
        "\n",
        "- 途中で落ちると、ollamaのプロセスが落ちるだけなので、そこだけ再起動すればよい、HDDにpullした結果が残っている？ので、再起動すると、すでにpullしたものはそのまま使えます。\n",
        "\n",
        "```\n",
        "# これはコードとして書式設定されます\n",
        "ollama run mistral\n",
        "ollama run llama3\n",
        "ollama run phi3\n",
        "```"
      ],
      "metadata": {
        "id": "2XAIvK5FhdJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8B(ビリオン、80億パラメター）モデルならば、Colab環境でどうにか動きます。\n",
        "\n",
        "とりあえず、llama2が無難なのでこれを使います、他のllmも試すとよいと思います。"
      ],
      "metadata": {
        "id": "aXshyzT5hlCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OLLAMA_MODEL='gemma:7b'\n",
        "OLLAMA_MODEL='mistral'\n",
        "#OLLAMA_MODEL='phi3'\n",
        "#OLLAMA_MODEL='wizardlm2'\n",
        "OLLAMA_MODEL='llama3'\n",
        "#OLLAMA_MODEL='llama2'\n",
        "\n",
        "# Set it at the OS level\n",
        "import os\n",
        "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
        "!echo $OLLAMA_MODEL"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:01:34.335475Z",
          "iopub.execute_input": "2024-03-06T16:01:34.335822Z",
          "iopub.status.idle": "2024-03-06T16:01:35.317101Z",
          "shell.execute_reply.started": "2024-03-06T16:01:34.335789Z",
          "shell.execute_reply": "2024-03-06T16:01:35.315949Z"
        },
        "trusted": true,
        "id": "Jcbl2R1iEQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM(例えばllama3)をローカル環境(ここではColabのマシン)にダウンロードします。"
      ],
      "metadata": {
        "id": "51Ejrgxzb-oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run $OLLAMA_MODEL \"Explain AI in one line\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:01:40.333790Z",
          "iopub.execute_input": "2024-03-06T16:01:40.334826Z",
          "iopub.status.idle": "2024-03-06T16:02:42.875644Z",
          "shell.execute_reply.started": "2024-03-06T16:01:40.334781Z",
          "shell.execute_reply": "2024-03-06T16:02:42.874462Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "9vjBLAFFEQRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要となるLangChainと関連ライブラリをインストールします。"
      ],
      "metadata": {
        "id": "XynppP-QcbsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet -U langchain langchainhub langchain_community"
      ],
      "metadata": {
        "id": "AAoQEqlXGrWi",
        "execution": {
          "iopub.status.busy": "2024-03-06T16:01:13.368041Z",
          "iopub.execute_input": "2024-03-06T16:01:13.368970Z",
          "iopub.status.idle": "2024-03-06T16:01:34.332571Z",
          "shell.execute_reply.started": "2024-03-06T16:01:13.368923Z",
          "shell.execute_reply": "2024-03-06T16:01:34.331141Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# langchinのバージョンを確認します、動作したのは0.2.1 (2023.5.25時)\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ],
      "metadata": {
        "id": "2ThRqglOdD-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 動作確認、Smoke Testです。"
      ],
      "metadata": {
        "id": "GSUn55hJFJNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "#OLLAMA_MODEL='mistral'\n",
        "llm = ChatOllama(model=OLLAMA_MODEL)\n",
        "\n",
        "llm.invoke(\"Who are you?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:07:20.437225Z",
          "iopub.execute_input": "2024-03-06T16:07:20.437603Z",
          "iopub.status.idle": "2024-03-06T16:07:20.442870Z",
          "shell.execute_reply.started": "2024-03-06T16:07:20.437572Z",
          "shell.execute_reply": "2024-03-06T16:07:20.441733Z"
        },
        "trusted": true,
        "id": "PAcacsovEQRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangchainをつかったRAG"
      ],
      "metadata": {
        "id": "mcH7mwITDKh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## まずはテキストを準備します"
      ],
      "metadata": {
        "id": "58HPqNaIkxqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trafilatura  sentence_transformers"
      ],
      "metadata": {
        "id": "_vEVeda5jLCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trafilatura import fetch_url, extract\n",
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "url = \"https://ja.m.wikipedia.org/wiki/ONE_PIECE\"\n",
        "filename = 'textfile.txt'\n",
        "\n",
        "document = fetch_url(url)\n",
        "text = extract(document)\n",
        "print(text[:1000])\n",
        "\n",
        "# 取ってきたテキストを一時的に保存\n",
        "with open(\"data/\"+filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "HB4uDFhBkzp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = TextLoader(\"data/\"+filename, encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "documents"
      ],
      "metadata": {
        "id": "2FzjFjD9jQVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].page_content"
      ],
      "metadata": {
        "id": "ups4IXAalFxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ドキュメントを、分割します。これは。LLMには入力データの長さ（コンテキスト長）に制限があるので、Q&Aにおいても元の参照データを一度にコンテキスト情報として与えられないという制約があります。\n",
        "\n",
        "そこで、chunkというサイズにテキストをバラバラにして、それをベクトル化データベースに格納します。\n",
        "\n",
        "まずは、chunk_size=500で分割します。"
      ],
      "metadata": {
        "id": "HHNnbQGYGfb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "ZATujRIJkjC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits)"
      ],
      "metadata": {
        "id": "_cjcmadVlQix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ちゃんと分割できてるかどうかを中身を見ます。"
      ],
      "metadata": {
        "id": "OWz8uOuAHLyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[30:33]"
      ],
      "metadata": {
        "id": "P2oFCVgzlU7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "APOXrHoalbVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "分割された文章の断片(chunk)を、ベクトル化データベース(ここではメタ社のFAISSを使います）に格納します。"
      ],
      "metadata": {
        "id": "_HbS8rkeGc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)"
      ],
      "metadata": {
        "id": "rSEd2yEalfbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollamaをつかって、LLMのオブジェクト(ollama)を作成し、ベクトルデータベースからQ&Aを行うqachainを作っておきます。"
      ],
      "metadata": {
        "id": "KHcCAUyvHkr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import Ollama\n",
        "\n",
        "ollama = Ollama(model=OLLAMA_MODEL)\n",
        "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 3},\n",
        "    ))"
      ],
      "metadata": {
        "id": "4XFpC3JVn_px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAGは\n",
        "\n",
        "1. ベクトルDBから質問に類似するchunkを類似検索により取得する\n",
        "2. 取得されたchunk(複数）を、入力コンテキストに積んで、これに対する質問プロンプトを作成し、LLMを呼ぶ。\n",
        "3. LLMは質問に対する回答を出力する\n",
        "\n",
        "という動きをします。最初にベクトルＤＢから質問に対する類似のチャンクを得ます。"
      ],
      "metadata": {
        "id": "BMoAONNkGaDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"ニコ・ロビンの職業は何ですか？\"\n",
        "question=\"エネルは何者ですか？\"\n",
        "#question=\"チョッパーの特殊能力は何ですか？\"\n",
        "#question=\"サンジは麦わらの一味に加わる前には何をしていたか？\"\n",
        "question=\"ワポルは何者であり、最後はどうなったか？\"\n",
        "docs = vectorstore.similarity_search(question)\n",
        "docs"
      ],
      "metadata": {
        "id": "FULFzqBGnRBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "得られた結果から、質問に対する回答をＬＬＭが出力します。"
      ],
      "metadata": {
        "id": "_JCzBJPvISf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qachain.invoke({\"query\": question})"
      ],
      "metadata": {
        "id": "rj5WNeEZvX3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果はどうでしょうか？↑のセルを起動するごとに異なる回答が生成されます。ＬＬＭによっては、出力が英語だったりします。"
      ],
      "metadata": {
        "id": "t_v-EeDEIhq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llamaindexでRAGの実現"
      ],
      "metadata": {
        "id": "yEqlrATkwt6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-llms-ollama llama_index.embeddings.huggingface llama_index.readers.wikipedia"
      ],
      "metadata": {
        "id": "ArcfpSLoD1MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "Settings.llm =  Ollama(model=OLLAMA_MODEL, request_timeout=60.0)\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    #model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "    model_name=\"intfloat/multilingual-e5-large\"\n",
        ")"
      ],
      "metadata": {
        "id": "SaOmhwq_wx1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# ドキュメントの読み込み\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "SNbcR-bcxOoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=3)"
      ],
      "metadata": {
        "id": "IKRZuo3Oyqli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"ニコ・ロビンの職業は何ですか？\"\n",
        "#question=\"エネルは何者ですか？\"\n",
        "question=\"チョッパーの特殊能力は何ですか？\"\n",
        "#question=\"サンジは麦わらの一味に加わる前には何をしていたか？\"\n",
        "question=\"ワポルは何者であり、最後はどうなったか？\"\n",
        "response = query_engine.query(question)\n",
        "display(response.response)"
      ],
      "metadata": {
        "id": "5wckojoczfRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベクトルＤＢから得られた類似チャンクを表示できます。"
      ],
      "metadata": {
        "id": "o8gs6NJeKSFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for node in response.source_nodes:\n",
        "  print(node.text)\n",
        "  print(node.score)"
      ],
      "metadata": {
        "id": "c7cD-i8FJQbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "プロンプトをカスタマイズできるらしい、\n",
        "\n",
        "https://lightning.ai/lightning-ai/studios/compare-llama-3-and-phi-3-using-rag?utm_source=akshay"
      ],
      "metadata": {
        "id": "eRGTTEbwHzvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str = (\n",
        "            \"コンテキスト情報は以下である.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"上記コンテキスト情報が与えられたとき、質問queryに対してステップbyステップに考察してAnswerを出力してほしい,出力は日本語で.\\n\"\n",
        "            \"Query: {query_str}\\n\"\n",
        "            \"Answer: \"\n",
        "            )\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
        "\n",
        "response = query_engine.query(question)\n",
        "display(response.response)"
      ],
      "metadata": {
        "id": "RRc4aDOXH6l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## おまけ、日本語向けのLLM(ELYZA)を使う\n",
        "\n",
        "いままでの例では、出力が日本語になりませんでした。そこで日本語向けに微調整された、モデルを使ってみます。\n",
        "\n",
        "- ELYZAの4bit量子化モデル(コンパクト化されたモデル）を利用(gguf形式）\n",
        "- ELYZAのModelfileは、llama2のものを利用\n",
        "- OllamaにELYZAのggufモデルを読み込ませる。\n",
        "- 普通に使う（モデル名を指定して）\n",
        "\n",
        "こちらを参考にしています。\n",
        "\n",
        "https://note.com/npaka/n/ndadbae6c6be5\n"
      ],
      "metadata": {
        "id": "dN7Gkyvnna9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Modelfile\n",
        "\n",
        "FROM ./ELYZA-japanese-Llama-2-7b-instruct-q4_K_M.gguf\n",
        "TEMPLATE \"\"\"[INST] <<SYS>>{{ .System }}<</SYS>>\n",
        "\n",
        "{{ .Prompt }} [/INST]\n",
        "\"\"\"\n",
        "PARAMETER stop \"[INST]\"\n",
        "PARAMETER stop \"[/INST]\"\n",
        "PARAMETER stop \"<<SYS>>\"\n",
        "PARAMETER stop \"<</SYS>>\""
      ],
      "metadata": {
        "id": "fsZ72lDzItiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-7b-instruct-q4_K_M.gguf"
      ],
      "metadata": {
        "id": "sJxaSeUvsUFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create elyza:7b-instruct -f Modelfile"
      ],
      "metadata": {
        "id": "0q93RKgRsau1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELYZAがollamaに認識されているか？"
      ],
      "metadata": {
        "id": "Gg5QufLvgHg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "aDz7IB10flt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm =  Ollama(model='elyza:7b-instruct', request_timeout=60.0)"
      ],
      "metadata": {
        "id": "ruIGBQMztcbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメントの読み込み\n",
        "#documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "62hLz3FntrbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"ニコ・ロビンの職業は何ですか？\"\n",
        "#question=\"エネルは何者ですか？\"\n",
        "#question=\"チョッパーの特殊能力は何ですか？\"\n",
        "#question=\"サンジは麦わらの一味に加わる前には何をしていたか？\"\n",
        "question=\"ワポルは何者であり、最後はどうなったか？\"\n",
        "response = query_engine.query(question)\n",
        "display(response.response)"
      ],
      "metadata": {
        "id": "edozkFBat6PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さらにプロンプトをカスタマイズしてみると。。"
      ],
      "metadata": {
        "id": "dxsRoW7OwRSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str = (\n",
        "            \"コンテキスト情報は以下である.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"上記コンテキスト情報が与えられたとき、質問queryに対してステップbyステップに考察してAnswerを出力してほしい,出力は日本語で.\\n\"\n",
        "            \"Query: {query_str}\\n\"\n",
        "            \"Answer: \"\n",
        "            )\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
        "\n",
        "response = query_engine.query(question)\n",
        "display(response.response)"
      ],
      "metadata": {
        "id": "J7kvG1aIwQfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72pAnwKfwjLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}